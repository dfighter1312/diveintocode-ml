{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8693b8",
   "metadata": {},
   "source": [
    "# Sprint 15\n",
    "\n",
    "## Treatise Reading\n",
    "\n",
    "**Paper:**\n",
    "Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks.In: Advances in neural information processing systems. (2015 ) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621031",
   "metadata": {},
   "source": [
    "### (1) What kind of method exists in the field of object detection?\n",
    "\n",
    "- R-CNN\n",
    "- SPPnets\n",
    "\n",
    "> (Abstract) Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a6477",
   "metadata": {},
   "source": [
    "### (2) It says \"Faster\", but what mechanism was used to make it faster?\n",
    "\n",
    "The mechanism is to reimplement the methods used in research for the GPU instead of CPU.\n",
    "\n",
    "> (1. Introduction) One may note that fast region-based CNNs take advantage of GPUs, while the region proposal meth\u0002ods used in research are implemented on the CPU, making such runtime comparisons inequitable. An obvious way to accelerate proposal computation is to reimplement it for the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa15294",
   "metadata": {},
   "source": [
    "### (3) How is the One-Stage method different from the Two-Stage method?\n",
    "\n",
    "One-stage refers to class-specific detection, while two-stage includes class-agnostic proposals and class-specific detection.\n",
    "\n",
    "> (4.1. Experiments on PASCAL VOC) One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3 × 3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster RCNN—the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdcdf5",
   "metadata": {},
   "source": [
    "### (4) What is RPN?\n",
    "\n",
    "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.\n",
    "\n",
    "To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer.\n",
    "\n",
    "> (3.1. Region Proposal Networks) A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33914e",
   "metadata": {},
   "source": [
    "### (5) What is RoI pooling?\n",
    "\n",
    "RoI polling is a layer in faster R-CNN.\n",
    "\n",
    "The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W.\n",
    "\n",
    "*(R. Girshick, “Fast R-CNN,” in IEEE International Conference on\n",
    "Computer Vision (ICCV), 2015.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12e85f",
   "metadata": {},
   "source": [
    "### (6) What is the proper size for Anchor?\n",
    "\n",
    "The proper size is using 3 scales (128, 256, 512) and 3 aspect ratios (2:1, 1:1, 1:2). An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.\n",
    "\n",
    "> (3.1.1. Anchor) An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.\n",
    "\n",
    "> (4.1 Experiments on PASCAL VOC) Table 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80daabda",
   "metadata": {},
   "source": [
    "### (7) What kind of data set is used and what kind of index value is obtained compared to the previous research?\n",
    "\n",
    "Datasets: PASCAL VOC 2007, 2012, and MS COCO datasets.\n",
    "\n",
    "> (Abstract) For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
