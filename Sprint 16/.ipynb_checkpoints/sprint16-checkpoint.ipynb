{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f21abe",
   "metadata": {},
   "source": [
    "# Sprint 16\n",
    "\n",
    "## Thesis Introduction\n",
    "\n",
    "Here, I selected **Named Entity Recognition** as the area of interest, since I really like working on Natural Language Processing and this problem is one of the tasks in my university, so I think I can combine it in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07394a",
   "metadata": {},
   "source": [
    "### 1. Li, Jing, et al. \"A survey on deep learning for named entity recognition.\" IEEE Transactions on Knowledge and Data Engineering (2020).\n",
    "\n",
    "**Why did I choose this paper?**\n",
    "\n",
    "For every problem, reading a survey is quite a natural instinct before looking for any innovative work. We can know what has been done to solve the problem, what are the challenges and future research direction.\n",
    "\n",
    "**Abstract the paper**\n",
    "\n",
    "- Named Entity Recognition (NER) is the task to identify text spans that mention named entites, and to classify them into predefined categories (e.g., person, location, organization, etc.).  NER serves as the basis for a variety of natural language applications such as question answering, text summarization, and machine translation.\n",
    "- Traditional approaches include:\n",
    "    - Rule-based Approaches:\n",
    "        - Pros: Works well when lexicon is exhaustive.\n",
    "        - Cons: cannot be transferred to other domains.\n",
    "    - Unsupervised learning Approaches:\n",
    "        - Pros: Reduce the work of labelling data.\n",
    "        - Cons: Need time to interpret the results.\n",
    "    - Feature-based Supervised Learning Approaches:\n",
    "        - Pros: Better generalization.\n",
    "        - Cons: Requires considerable amount of engineering skill and domain expertise.\n",
    "- Survey in Deep Learning techniques is the main contribution of this paper. It showed a new taxonomy of DL-based NER.\n",
    "![image](fig1.png)\n",
    "- Core strengths of Deep Learning approaches:\n",
    "    - Benefits from the non-linear transformation, which generates non-mappings from input to output.\n",
    "    - Save effort on designing NER features, compare to feature-based approahces.\n",
    "    - Deep neural NER models can be trained in an end-to-end paradigm, by gradient descent, hence enable us to design possibly complex NER system.\n",
    "- Architectures of every component in taxonomy:\n",
    "    - Distributed Representation for Input: one-hot vector representation, word-level representation, character-level representation (mostly use CNN-based and RNN-based), hybrid representation, etc.\n",
    "    - Context encoder: CNN, RNN, recursive NN, deep transformer, neural language model\n",
    "    - Tag encoder: MLP + Softmax, Conditional Random Fields (CRF), RNN, Pointer Networks.\n",
    "- Comparison:\n",
    "    - Work with highest F1-score for CoNLL03 dataset used pre-trains and bidirectional transformer model in a close-style manner\n",
    "    - Work with highest F1-score for OntoNotes5.0 dataset used BERT and dice loss.\n",
    "- The most representative methods for recent applied techniques of deep learning in new NER problem settings and applications:\n",
    "    - Deep Multi-task Learning\n",
    "    - Deep Transfer Learning\n",
    "    - Deep Active Learning\n",
    "    - Deep Adversarial Learning\n",
    "    - Deep Reinforcement Learning\n",
    "    - Neural Attention\n",
    "- Challenges faced by NER systems:\n",
    "    - Data Annotation\n",
    "    - Informal Text and Unseen Entities\n",
    "- Future directions:\n",
    "    - Fine-grained NER and Boundary Detection\n",
    "    - Joint NER and Entity Linking\n",
    "    - DL-based NER on Informal Text with Auxiliary Resource\n",
    "    - Scalability of DL-based NER\n",
    "    - Deep Transfer Learning for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8e126",
   "metadata": {},
   "source": [
    "### 2. Kim, Ji-Hwan, and Philip C. Woodland. \"A rule-based named entity recognition system for speech input.\" Sixth International Conference on Spoken Language Processing. 2000.\n",
    "\n",
    "**Why did I chose this paper?**\n",
    "\n",
    "This is the representative of rule-based approaches in NER.\n",
    "\n",
    "**Abstract the paper**\n",
    "\n",
    "- Propse a rule based (transformation based) NER system which uses the Brill rule inference approach.\n",
    "    - The preprocessing includes: Add word features and look-up name lists.\n",
    "    - The rule-generation includes: Generate applcable rules, update environments, find the best rule and update NE labels in training data.\n",
    "![image](fig2.png)\n",
    "- Compare the performance between the proposed method and IdentiFinder, one of the most successful stochastic systems.\n",
    "    - In the baseline case (no punctuation and no captialisation), both systems show almost equal performance.\n",
    "    - The performance of both systems degrade linearly with added speech recognition errors, and almost equal.\n",
    "    - They conclude that automatic rule inference is a viable alternative to the HMM-based approach to named entity recognition, but it retains the advantages of a rule-based approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70344e8",
   "metadata": {},
   "source": [
    "### 3. D. M. Bikel, R. Schwartz, and R. M. Weischedel, “An algorithm that learns what’s in a name,” Mach. Learn., vol. 34, no. 1-3, pp. 211–231, 1999.\n",
    "\n",
    "**Why did I choose this paper?**\n",
    "\n",
    "This is the representative of feature-based machine learning approaches in NER. Also, paper (2) made a comparison to this one.\n",
    "\n",
    "**Abstract the paper**\n",
    "\n",
    "- Present IdentiFinder, a hidden Markov model that learns to solve NER problems.\n",
    "- The representation of HMM model: ![image](fig3.png)\n",
    "    - States include: NOT-A-NAME, ORGANIZATION, PERSON and specially START-OF-SENTENCE and END-OF-SENTENCE.\n",
    "    - Every word is represented by a state in the bigram model, and there is a probability associated with every transition from the current word to the next word.\n",
    "- Adding word-features for each word. Therefore words are considered to be ordered pairs <w, f>. Words features slightly increased the performance during the experiment.\n",
    "- Implement the model in C++, and evaluate the model in English, Spanish and speech input, which IndetiFinder got performance around 90% on newswire. IdentiFinder’s performance is also competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e41d93",
   "metadata": {},
   "source": [
    "### 4. P. Zhou, S. Zheng, J. Xu, Z. Qi, H. Bao, and B. Xu, “Joint extraction of multiple relations and entities by using a hybrid neural network,” in CCL-NLP-NABD. Springer, 2017, pp. 135–146.\n",
    "\n",
    "**Why did I chose this paper?**\n",
    "\n",
    "This is one of the representatives of deep learning approaches. Also, I had some knowledge about the architecture that they used.\n",
    "\n",
    "**Abstract the paper**\n",
    "- Proposed model uses a hybrid neural network to automatically learn sentence features and does not rely on any Natural Language Processing (NLP) tools, such as dependency parser.\n",
    "- The architecture consists of 5 components: Input Layer, Embedding Layer, BLSTM Layer (Bidirectional LSTM layer), RC Module and NER Module. ![image](fig4.png)\n",
    "    - Input Layer\n",
    "    - Embedding Layer: For each word in s, we first look up the embedding matrix. Then we transform a word into its word embedding using the matrix-vector product. Then the sentence is fed to the next layer as a real-valued matrix.\n",
    "    - BLSTM layer: LSTM was proposed to overcome the gradient vanishing problem of RNN, and BLSTM was proposed to extend the unidirectional LSTM by introducing a second hidden layer, where the hidden to hidden connections flow in the opposite temporal order. Therefore, BLSTM can exploit information from both the past and the future.\n",
    "    - Relation Classification Module: Consists of:\n",
    "        - Convolution Layer\n",
    "        - Max Pooling Layer\n",
    "        - Sigmoid Activation Layer\n",
    "    - Named Entity Recognition Module: Consists of\n",
    "        - LSTM Decoder\n",
    "        - Softmax Activation Layer\n",
    "- Experiments on the CoNLL04 dataset demonstrate that our model using only word embeddings as input features achieves state-of-the-art performance.\n",
    "- Analyze the effect of sentence length and relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dac108",
   "metadata": {},
   "source": [
    "### 5. X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, and J. Li, “Dice loss for data-imbalanced NLP tasks,” CoRR, vol. abs/1911.02855, 2019.\n",
    "\n",
    "**Why did I choose this paper?**\n",
    "\n",
    "- This is one of the representatives of deep learning approaches.\n",
    "- State-of-the-art mentioned by the survey (1).\n",
    "\n",
    "**Abstract the paper**\n",
    "- Data imbalance is the issue occurred in many NLP tasks: negative examples significantly outnumber positive ones, the huge number of easy-negative examples overwhelms training.\n",
    "- Propose to use dice loss in replacement of the standars cross-entropy objective which is accutually accuracy-oriented and creates a discrepancy between training and test. The dice loss is F1-oriented.\n",
    "- Experiment on speech tagging and named entity recognition. All experiments observed an increase in F1-score when using the dice loss compare to other loss including cross entropy, weighted cross entropy, dice coefficient and focal loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
