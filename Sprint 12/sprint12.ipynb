{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1ed015",
   "metadata": {},
   "source": [
    "# Sprint 12\n",
    "\n",
    "# SimpleConv2d\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2eaf9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1d5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, *shape):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(*shape)\n",
    "        return W\n",
    "    \n",
    "    def B(self, *shape):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3070e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc0baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf0a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        layer.HW += layer.dW * layer.dW\n",
    "        layer.HB = layer.dB * layer.dB\n",
    "        delta = 1e-7\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + delta)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.HB) + delta)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4778a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90393540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.Z = np.exp(A - np.max(A)) / np.sum(np.exp(A - np.max(A)), axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eea876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.A = np.dot(self.X, self.W) + self.B\n",
    "        return self.activation.forward(self.A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        dA = self.activation.backward(dZ)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.X.T, dA) / len(self.X)\n",
    "        self.dZ = np.dot(dA, self.W.T)\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15647384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random number seed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4d6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializerConv2d:\n",
    "    \n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, F, C, FH, FW):\n",
    "        return self.sigma * np.random.randn(F, C, FH, FW)\n",
    "    \n",
    "    def B(self, F):\n",
    "        return np.zeros(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd37148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(48000,)\n",
      "(12000, 28, 28)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47188f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e5d9d",
   "metadata": {},
   "source": [
    "## 【Problem 1】Creating a 2-D convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1306148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv2d():\n",
    "    \"\"\"\n",
    "    2D convolutional layer\n",
    "    Parameters\n",
    "    -------\n",
    "    F: Number of output channels\n",
    "    C: Number of input channels\n",
    "    FH: Filter's height.\n",
    "    FC: Filter's width.\n",
    "    P: Padding.\n",
    "    S: Stride.\n",
    "    initializer: Instances of initialization methods\n",
    "    optimizer: Instances of optimization methods\n",
    "    \"\"\"\n",
    "    def __init__(self, F, C, FH, FW, P=0, S=1, initializer=None, optimizer=None, activation=None):\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initializer W and B with initializer\n",
    "        self.W = self.initializer.W(F, C, FH, FW)\n",
    "        self.B = self.initializer.B(F)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        Parameters\n",
    "        -------\n",
    "        X: ndarray\n",
    "            Input data\n",
    "        Returns\n",
    "        -------\n",
    "        A: ndarray\n",
    "            Output data\n",
    "        \"\"\"\n",
    "        # (n_samples, n_channels, height, width)\n",
    "        N, C, H, W = X.shape\n",
    "        F, C, FH, FW = self.W.shape\n",
    "        \n",
    "        OH, OW = self.output_shape_2d(H, W, self.P, self.P, FH, FW, self.S, self.S)\n",
    "\n",
    "        self.params = N, C, H, W, F, FH, FW, OH, OW\n",
    "        \n",
    "        A = np.zeros([N, F, OH, OW])\n",
    "        \n",
    "        self.X_pad = np.pad(X, ((0,0), (0,0), (self.P, self.P), (self.P, self.P)))\n",
    "    \n",
    "        # For each sample\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(0, OH, self.S):\n",
    "                    # Horizontal slide\n",
    "                    for col in range(0, OW, self.S):\n",
    "                        A[n, ch, row, col] = np.sum(self.X_pad[n, :, row:row+FH, col:col+FW] * self.W[ch, :, :, :]) + self.B[ch]\n",
    "        \n",
    "        if self.activation == None:\n",
    "            return A\n",
    "        else:\n",
    "            return self.activation.forward(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Backward method.\n",
    "        Parameters\n",
    "        -------\n",
    "        dZ: \n",
    "            The gradient flown in from behind.\n",
    "        Returns\n",
    "        -------\n",
    "        dZ: ndarray\n",
    "            Forward slope\n",
    "        \"\"\"\n",
    "        dA = dZ if self.activation == None else self.activation.backward(dZ)\n",
    "        N, C, H, W, F, FH, FW, OH, OW = self.params\n",
    "        \n",
    "        dZ = np.zeros(self.X_pad.shape)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "        \n",
    "        # Calculate dZ\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(0, OH, self.S):\n",
    "                    for col in range(0, OW, self.S):\n",
    "                        dZ[n, :, row:row+FH, col:col+FW] += dA[n, ch, row, col] * self.W[ch, :, :, :]\n",
    "        \n",
    "        dl_rows = range(self.P), range(H + self.P, H + 2*self.P, 1)\n",
    "        dl_cols = range(self.P), range(W + self.P, W + 2*self.P, 1)\n",
    "        \n",
    "        dZ = np.delete(dZ, dl_rows, axis=2)\n",
    "        dZ = np.delete(dZ, dl_cols, axis=3)\n",
    "        \n",
    "        # Calculate dW\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(OH):\n",
    "                    for col in range(OW):\n",
    "                        self.dW[ch, :, :, :] += dA[n, ch, row, col] * self.X_pad[n, :, row:row+FH, col:col+FW]\n",
    "        \n",
    "        # Calculate dB\n",
    "        for ch in range(F):\n",
    "            self.dB[ch] = np.sum(dA[:, ch, :, :])\n",
    "            \n",
    "        # Update\n",
    "        if self.optimizer is not None:    \n",
    "            self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def output_shape_2d(self, H, W, PH, PW, FH, FW, SH, SW):\n",
    "        OH = (H + 2*PH - FH) / SH + 1\n",
    "        OW = (W + 2*PW - FW) / SW + 1\n",
    "        return int(OH), int(OW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a99787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantInitializer():\n",
    "    \n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def W(self, *args):\n",
    "        return self.w\n",
    "    \n",
    "    def B(self, *args):\n",
    "        return self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063e7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1,1,4,4)\n",
    "x = np.array([[[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]]])\n",
    "# (2,1,3,3)\n",
    "w = np.array([[[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]]],\n",
    "              [[[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]]])\n",
    "\n",
    "b = np.array([0, 0])\n",
    "\n",
    "test = SimpleConv2d(1, 1, 3, 3, initializer=ConstantInitializer(w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0823f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-4., -4.],\n",
       "         [-4., -4.]],\n",
       "\n",
       "        [[ 1.,  1.],\n",
       "         [ 1.,  1.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test.forward(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f0d430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0.,   0.,   0.,   0.],\n",
       "         [  0.,  -5.,   4.,  -7.],\n",
       "         [  0.,  13.,  27., -11.],\n",
       "         [  0., -10., -11.,   0.]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = np.array([[[[ -4,  -4],\n",
    "                   [ 10,  11]],\n",
    "                  [[  1,  -7],\n",
    "                   [  1, -11]]]])\n",
    "\n",
    "back_result = test.backward(delta)\n",
    "back_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872096df",
   "metadata": {},
   "source": [
    "## [Problem 4] Creating a maximum pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0533394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    \n",
    "    def __init__(self, P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pidx = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        N, F, OH, OW = A.shape\n",
    "        PS = self.P\n",
    "        PH, PW = int(OH/PS), int(OW/PS)\n",
    "        \n",
    "        self.params = N, F, OH, OW, PS, PH, PW\n",
    "        \n",
    "        # Pooling filter\n",
    "        self.PA = np.zeros((N, F, PH, PW))\n",
    "        self.Pidx = np.zeros((N, F, PH, PW))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n, ch, row, col] = np.max(A[n, ch, row*PS:row*PS+PS, col*PS:col*PS+PS])\n",
    "                        self.Pidx[n, ch, row, col] = np.argmax(A[n, ch, row*PS:row*PS+PS, col*PS:col*PS+PS])\n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        N, F, OH, OW, PS, PH, PW = self.params\n",
    "        dP = np.zeros((N, F, OH, OW))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        idx = self.Pidx[n, ch, row, col]\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n, ch, row, col]\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "                        dP[n, ch, row*PS:row*PS+PS, col*PS:col*PS+PS] = tmp.reshape(PS, PS)\n",
    "        return dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8208bae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[[\n",
    "    [3, 6, 0, 3, 1, 0],\n",
    "    [6, 8, 3, 4, 1, 0],\n",
    "    [7, 8, 6, 6, 8, 2],\n",
    "    [7, 6, 4, 4, 0, 2],\n",
    "    [0, 6, 6, 1, 5, 7],\n",
    "    [0, 5, 5, 4, 7, 7]\n",
    "]]])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb705db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[8., 4., 1.],\n",
       "         [8., 6., 8.],\n",
       "         [6., 6., 7.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling = MaxPool2D(P=2)\n",
    "A = pooling.forward(X)\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06fedc60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3., 3., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 1.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling.Pidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ba18627",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e9786e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., 0., 1., 0.],\n",
       "         [0., 8., 0., 4., 0., 0.],\n",
       "         [0., 8., 6., 0., 8., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 6., 6., 0., 0., 7.],\n",
       "         [0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ = pooling.backward(dA)\n",
    "dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf002e",
   "metadata": {},
   "source": [
    "## [Problem 5] Create Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e047a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool2D():\n",
    "    \n",
    "    def __init__(self, P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pidx = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        N, F, OH, OW = A.shape\n",
    "        PS = self.P\n",
    "        PH, PW = int(OH/PS), int(OW/PS)\n",
    "        \n",
    "        self.params = N, F, OH, OW, PS, PH, PW\n",
    "        \n",
    "        # Pooling filter\n",
    "        self.PA = np.zeros((N, F, PH, PW))\n",
    "        self.Pidx = np.zeros((N, F, PH, PW))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n, ch, row, col] = np.mean(A[n, ch, row*PS:row*PS+PS, col*PS:col*PS+PS])\n",
    "                        \n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        N, F, OH, OW, PS, PH, PW = self.params\n",
    "        dP = np.zeros((N, F, OH, OW))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            tmp[i] = dA[n, ch, row, col] / (PS*PS)\n",
    "                        dP[n, ch, row*PS:row*PS+PS, col*PS:col*PS+PS] = tmp.reshape(PS, PS)\n",
    "        return dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471c7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[\n",
    "    [2, 1, 2, 3, 0, 2],\n",
    "    [4, 4, 8, 8, 8, 0],\n",
    "    [5, 8, 0, 8, 7, 0],\n",
    "    [8, 3, 0, 0, 0, 5],\n",
    "    [0, 8, 3, 0, 5, 7],\n",
    "    [4, 8, 3, 6, 7, 6]\n",
    "]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90300f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[2.75, 5.25, 2.5 ],\n",
       "         [6.  , 2.  , 3.  ],\n",
       "         [5.  , 3.  , 6.25]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling = AveragePool2D(P=2)\n",
    "A = pooling.forward(X)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "648ad970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = np.array([[[\n",
    "    [2, 6, 6],\n",
    "    [6, 5, 3],\n",
    "    [3, 6, 5]\n",
    "]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "711662c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.5 , 0.5 , 1.5 , 1.5 , 1.5 , 1.5 ],\n",
       "         [0.5 , 0.5 , 1.5 , 1.5 , 1.5 , 1.5 ],\n",
       "         [1.5 , 1.5 , 1.25, 1.25, 0.75, 0.75],\n",
       "         [1.5 , 1.5 , 1.25, 1.25, 0.75, 0.75],\n",
       "         [0.75, 0.75, 1.5 , 1.5 , 1.25, 1.25],\n",
       "         [0.75, 0.75, 1.5 , 1.5 , 1.25, 1.25]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ = pooling.backward(dA)\n",
    "dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135e7ea",
   "metadata": {},
   "source": [
    "## [Problem 6] Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "465ddcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X), -1)\n",
    "    \n",
    "    def backward(self, X):\n",
    "        return X.reshape(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "870e5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((10, 20, 30, 50))\n",
    "flat = Flatten()\n",
    "flat_forward = flat.forward(X)\n",
    "flat_forward.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "417a5b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 30, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat.backward(flat_forward).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd7011",
   "metadata": {},
   "source": [
    "## [Problem 7] Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad0d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Scratch2dCNNClassifier():\n",
    "    \n",
    "    def __init__(self, NN, CNN, n_epoch=5, n_batch=1, verbose=False):\n",
    "        self.NN = NN\n",
    "        self.CNN = CNN\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.verbose = verbose\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accs = []\n",
    "        self.val_accs = []\n",
    "        \n",
    "    def loss_function(self, y, yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "    \n",
    "    def accuracy(self, Z, Y):\n",
    "        return accuracy_score(Y, Z)\n",
    "    \n",
    "    def fb(self, X, y, batch=1):\n",
    "        \"\"\"\n",
    "        Do forward and backward, packaging in a function.\n",
    "        \"\"\"\n",
    "        forward_data = X[:, np.newaxis, :, :]\n",
    "\n",
    "        # Do forward propagation\n",
    "        for layer in self.CNN:\n",
    "            forward_data = layer.forward(forward_data)\n",
    "\n",
    "        fit = Flatten()\n",
    "        forward_data = fit.forward(forward_data)\n",
    "        for layer in self.NN:\n",
    "            forward_data = layer.forward(forward_data)\n",
    "            \n",
    "        # Predicted value\n",
    "        Z = forward_data\n",
    "        # Do back propagation\n",
    "        backward_data = (Z - y) / batch\n",
    "        for layer in reversed(self.NN):\n",
    "            backward_data = layer.backward(backward_data)\n",
    "\n",
    "        backward_data = fit.backward(backward_data)\n",
    "\n",
    "        for layer in reversed(self.CNN):\n",
    "            backward_data = layer.backward(backward_data)\n",
    "            \n",
    "        loss = self.loss_function(Z, y)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Train a neural network classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -------\n",
    "        X: ndarray\n",
    "            Training data.\n",
    "        y: ndarray\n",
    "            Target values of training data.\n",
    "        X_val: ndarray\n",
    "            Validation data.\n",
    "        y_val: ndarray\n",
    "            Target values of validation data.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.n_epoch):\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            self.loss = 0\n",
    "            self.val_loss = 0\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                # Forward\n",
    "                forward_data = mini_X_train[:, np.newaxis, :, :]\n",
    "                \n",
    "                for layer in range(len(self.CNN)):\n",
    "                    forward_data = self.CNN[layer].forward(forward_data)\n",
    "                \n",
    "                # Flatten\n",
    "                fit = Flatten()\n",
    "                forward_data = fit.forward(forward_data)\n",
    "                \n",
    "                # NN\n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "                \n",
    "                Z = forward_data\n",
    "                \n",
    "                # Backward\n",
    "                backward_data = (Z - mini_y_train)/self.n_batch\n",
    "                for layer in range(len(self.NN)-1, -1, -1):\n",
    "                    backward_data = self.NN[layer].backward(backward_data)\n",
    "                    \n",
    "                backward_data = fit.backward(backward_data)\n",
    "                \n",
    "                for layer in range(len(self.CNN)-1, -1, -1):\n",
    "                    backward_data = self.CNN[layer].backward(backward_data)\n",
    "                \n",
    "                self.loss += self.loss_function(Z, mini_y_train)\n",
    "            \n",
    "            loss = self.loss/len(get_mini_batch)\n",
    "            acc = self.accuracy(self.predict(X), np.argmax(y, axis=1))\n",
    "            print(f\"loss: {loss}, acc: {acc}\")\n",
    "            self.losses.append(loss)\n",
    "#             self.val_losses.append(self.val_loss/len(get_mini_batch))\n",
    "            self.accs.append(acc)\n",
    "#             if X_val is not None and y_val is not None:\n",
    "#                 self.val_accs.append(self.accuracy(self.predict(X_val), np.argmax(y_val, axis=1)))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        pred_data = X[:, np.newaxis, :, :]\n",
    "        \n",
    "        for layer in self.CNN:\n",
    "            pred_data = layer.forward(pred_data)\n",
    "            \n",
    "        fit = Flatten()\n",
    "        pred_data = fit.forward(pred_data)\n",
    "        \n",
    "        for layer in self.NN:\n",
    "            pred_data = layer.forward(pred_data)\n",
    "        \n",
    "        return np.argmax(pred_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1de6657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = [\n",
    "    FC(7840, 400, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    FC(400, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "273b517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = [\n",
    "    SimpleConv2d(F=10, C=1, FH=3, FW=3, P=1, S=1,\n",
    "                 initializer=SimpleInitializerConv2d(),\n",
    "                 optimizer=SGD(0.01),\n",
    "                 activation=ReLU())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dad27a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "loss: 0.22712575548772357, acc: 0.423\n",
      "Epoch 2\n",
      "loss: 0.23600258337523855, acc: 0.104\n",
      "Epoch 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-1b8d03de380a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcnn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratch2dCNNClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcnn1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_one_hot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-3747ce7a4e3a>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mini_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loss: {loss}, acc: {acc}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-3747ce7a4e3a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mpred_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-1fe447918818>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     55\u001b[0m                     \u001b[1;31m# Horizontal slide\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                         \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_pad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFW\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2228\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2229\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn1 = Scratch2dCNNClassifier(NN, CNN, n_epoch=10, n_batch=200, verbose=False)\n",
    "\n",
    "cnn1.fit(X_train[:1000], y_train_one_hot[:1000], X_val[:50], y_val_one_hot[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn1.predict(X_train[:1000])\n",
    "print(classification_report(y_train[:1000], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = cnn1.predict(X_val)\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde88886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1, cnn1.n_epoch+1), cnn1.losses, label='train_loss')\n",
    "plt.plot(np.arange(1, cnn1.n_epoch+1), cnn1.val_losses, label='val_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(1, cnn1.n_epoch+1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = cnn1.predict(X_train[:1000])\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447cd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train[:1000], y_pred_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
