{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4379cab2",
   "metadata": {},
   "source": [
    "# Deep Learning-based for Vietnamese Topic Modeling\n",
    "\n",
    "## PhoBERT + CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c249fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from vncorenlp import VnCoreNLP\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a3b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dbde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./csv/train.csv\", encoding='utf-16')\n",
    "test_df = pd.read_csv(\"./csv/test.csv\", encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f5bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[\"text\"]\n",
    "y_train = train_df[[\"label\"]]\n",
    "X_test = test_df[\"text\"]\n",
    "y_test = test_df[[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de721dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the labels\n",
    "encoder = OneHotEncoder()\n",
    "y_train_one_hot = encoder.fit_transform(y_train).toarray()\n",
    "# It is possible to directly transform y_test since the label sets are the same\n",
    "y_test_one_hot = encoder.transform(y_test).toarray()\n",
    "\n",
    "y_train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfa6ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33759, 1) (33759, 10)\n",
      "(50373, 1) (50373, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_train_one_hot.shape)\n",
    "print(y_test.shape, y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604bdb46",
   "metadata": {},
   "source": [
    "### Preprocessing on a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988c7f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thành lập dự án POLICY phòng chống HIV/AIDS ở VN (NLĐ)- Quỹ hỗ trợ khẩn cấp về AIDS của Hoa Kỳ vừa thành lập dự án POLICY tại VN với cam kết hỗ trợ Chính phủ và nhân dân VN đối phó HIV/AIDS.Dự án có nhiệm vụ chính là cải thiện công tác phòng chống HIV/AIDS thông qua các lĩnh vực xây dựng chính sách, rà soát các văn bản pháp luật, xây dựng chiến lược quảng bá, xây dựng chương trình đào tạo về phòng chống HIV/AIDS, lên kế hoạch bố trí nguồn lực, huấn luyện và nghiên cứu về phương tiện truyền thông đại chúng, tổ chức các hoạt động nhằm giảm kỳ thị và phân biệt đối xử đối với người có HIV/AIDS... Theo TTXVN, dự án POLICY đặc biệt quan tâm đến công tác truyền thông phòng chống HIV/AIDS, coi đây là một biện pháp tích cực và hữu hiệu trong việc phòng chống có hiệu quả HIV/AIDS. Thời gian tới, dự án POLICY sẽ tiếp tục tổ chức các hoạt động nhằm nâng cao nhận thức cho những người có trách nhiệm với công tác chỉ đạo phòng chống HIV/AIDS.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample text for testing\n",
    "text = X_train[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f478640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "    text = text.lower().strip()\n",
    "    return re.sub('[!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d03281a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thành lập dự án policy phòng chống hiv aids ở vn  nlđ   quỹ hỗ trợ khẩn cấp về aids của hoa kỳ vừa thành lập dự án policy tại vn với cam kết hỗ trợ chính phủ và nhân dân vn đối phó hiv aids dự án có nhiệm vụ chính là cải thiện công tác phòng chống hiv aids thông qua các lĩnh vực xây dựng chính sách  rà soát các văn bản pháp luật  xây dựng chiến lược quảng bá  xây dựng chương trình đào tạo về phòng chống hiv aids  lên kế hoạch bố trí nguồn lực  huấn luyện và nghiên cứu về phương tiện truyền thông đại chúng  tổ chức các hoạt động nhằm giảm kỳ thị và phân biệt đối xử đối với người có hiv aids    theo ttxvn  dự án policy đặc biệt quan tâm đến công tác truyền thông phòng chống hiv aids  coi đây là một biện pháp tích cực và hữu hiệu trong việc phòng chống có hiệu quả hiv aids  thời gian tới  dự án policy sẽ tiếp tục tổ chức các hoạt động nhằm nâng cao nhận thức cho những người có trách nhiệm với công tác chỉ đạo phòng chống hiv aids \n"
     ]
    }
   ],
   "source": [
    "text = text_normalization(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d625a1",
   "metadata": {},
   "source": [
    "Word segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ccf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8397b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thành_lập', 'dự_án', 'policy', 'phòng_chống', 'hiv', 'aids', 'ở', 'vn', 'nlđ', 'quỹ', 'hỗ_trợ', 'khẩn_cấp', 'về', 'aids', 'của', 'hoa_kỳ', 'vừa', 'thành_lập', 'dự_án', 'policy', 'tại', 'vn', 'với', 'cam_kết', 'hỗ_trợ', 'chính_phủ', 'và', 'nhân_dân', 'vn', 'đối_phó', 'hiv', 'aids', 'dự_án', 'có', 'nhiệm_vụ', 'chính', 'là', 'cải_thiện', 'công_tác', 'phòng_chống', 'hiv', 'aids', 'thông_qua', 'các', 'lĩnh_vực', 'xây_dựng', 'chính_sách', 'rà_soát', 'các', 'văn_bản', 'pháp_luật', 'xây_dựng', 'chiến_lược', 'quảng_bá', 'xây_dựng', 'chương_trình', 'đào_tạo', 'về', 'phòng_chống', 'hiv', 'aids', 'lên', 'kế_hoạch', 'bố_trí', 'nguồn_lực', 'huấn_luyện', 'và', 'nghiên_cứu', 'về', 'phương_tiện', 'truyền_thông', 'đại_chúng', 'tổ_chức', 'các', 'hoạt_động', 'nhằm', 'giảm', 'kỳ_thị', 'và', 'phân_biệt', 'đối_xử', 'đối_với', 'người', 'có', 'hiv', 'aids', 'theo', 'ttxvn', 'dự_án', 'policy', 'đặc_biệt', 'quan_tâm', 'đến', 'công_tác', 'truyền_thông', 'phòng_chống', 'hiv', 'aids', 'coi', 'đây', 'là', 'một', 'biện_pháp', 'tích_cực', 'và', 'hữu_hiệu', 'trong', 'việc', 'phòng_chống', 'có', 'hiệu_quả', 'hiv', 'aids', 'thời_gian', 'tới', 'dự_án', 'policy', 'sẽ', 'tiếp_tục', 'tổ_chức', 'các', 'hoạt_động', 'nhằm', 'nâng', 'cao', 'nhận_thức', 'cho', 'những', 'người', 'có', 'trách_nhiệm', 'với', 'công_tác', 'chỉ_đạo', 'phòng_chống', 'hiv', 'aids']]\n"
     ]
    }
   ],
   "source": [
    "sentences = rdrsegmenter.tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961649f",
   "metadata": {},
   "source": [
    "Testing PhoBERT in a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c6a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "537f2f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd706304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.roberta.modeling_tf_roberta.TFRobertaModel at 0x257a126cc40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a703ba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encoder['thành_lập']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7471122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0   763   169     3  2137     3     3    25 33756     3  1425   291\n",
      "  2498    28     3     7     3   164   763   169     3    35 33756    15\n",
      "  1253   291   926     6   603 33756  2971     3     3   169    10   527\n",
      "   159     8  1238   247  2137     3     3   652     9   518   141   502\n",
      "  1851     9   909   584   141   995  2524   141   270   643    28  2137\n",
      "     3     3    72   421  1681  2684  2019     6   410    28   715  1062\n",
      "  9847   116     9   132   272   197 12922     6  3043  4622   190    18\n",
      "    10     3     3    63     3   169     3   234   511    30   247  1062\n",
      "  2137     3     3   774]\n"
     ]
    }
   ],
   "source": [
    "tokens = [tokenizer.encode(sentences[0])]\n",
    "tokens = pad_sequences(tokens, maxlen=100, truncating=\"post\", padding=\"post\")\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa85a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.2505224  -0.35192013 -0.17607398 ...  0.03394873  0.38515067\n",
      "   -0.17919447]\n",
      "  [-0.14210458 -0.37149692 -0.40287185 ... -0.39811784 -0.15626681\n",
      "    0.18297672]\n",
      "  [-0.1855766  -0.29924142 -0.03049095 ...  0.2942223   0.22270073\n",
      "    0.15344876]\n",
      "  ...\n",
      "  [-0.14341971 -0.03227279 -0.59179056 ... -0.07983431  0.65585667\n",
      "   -0.5803716 ]\n",
      "  [-0.31038564 -0.21429403 -0.5138396  ... -0.12822092  0.46492967\n",
      "   -0.2527942 ]\n",
      "  [-0.3902443   0.6176958  -0.07049552 ... -0.13137478  0.24289557\n",
      "   -0.16345362]]], shape=(1, 100, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "document_matrix = phobert([tokens[:10]])\n",
    "print(document_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca5c354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorizer(dataset, maxlen=200):\n",
    "    dataset = dataset.tolist()\n",
    "    dataset = map(lambda x: text_normalization(x), dataset)\n",
    "    sentences = [rdrsegmenter.tokenize(x) for x in dataset]\n",
    "    tokens = [tokenizer.encode(sentence[0]) for sentence in sentences]\n",
    "    tokens = pad_sequences(tokens, maxlen=maxlen, truncating=\"post\", padding=\"post\")\n",
    "    vect = phobert(tokens).last_hidden_output\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = text_vectorizer(X_train)\n",
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79c009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_vect = text_vectorizer(X_test)\n",
    "X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6be07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(filters, maxlen=200):\n",
    "  \n",
    "    # Channel 1D CNN\n",
    "    inp = tf.keras.Input(shape=(maxlen, 768))\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=filters, kernel_size=2, activation='relu')(inp)\n",
    "    pool1 = tf.keras.layers.GlobalMaxPool1D()(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=filters, kernel_size=3, activation='relu')(inp)\n",
    "    pool2 = tf.keras.layers.GlobalMaxPool1D()(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=filters, kernel_size=4, activation='relu')(inp)\n",
    "    pool3 = tf.keras.layers.GlobalMaxPool1D()(conv3)\n",
    "    concat = tf.concat([pool1, pool2, pool3], axis=1)\n",
    "    dense1 = tf.keras.layers.Dense(filters * 2, activation='relu')(concat)\n",
    "    dense2 = tf.keras.layers.Dense(filters * 2, activation='relu')(dense1)\n",
    "    \n",
    "    # Interpretation\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(dense2)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model(20)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d31e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_vect,\n",
    "    y_train_one_hot,\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_one_hot = model.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = encoder.inverse_transform(y_pred_one_hot)\n",
    "print(y_pred[:10])\n",
    "print(y_test[:10].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d05ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
